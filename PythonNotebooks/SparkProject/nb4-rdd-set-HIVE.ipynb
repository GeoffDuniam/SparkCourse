{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set operations on RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Introduction to Spark with Python, by Jose A. Dianes](https://github.com/jadianes/spark-py-notebooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark supports many of the operations we have in mathematical sets, such as union and intersection, even when the RDDs themselves are not properly sets. It is important to note that these operations require that the RDDs being operated on are of the same type.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set operations are quite straightforward to understand as it work as expected. The only consideration comes from the fact that RDDs are not real sets, and therefore operations such as the union of RDDs doesn't remove duplicates. In this notebook we will have a brief look at `subtract`, `distinct`, and `cartesian`.       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data and creating the RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did in our first notebook, we will use the reduced dataset (10 percent) provided for the KDD Cup 1999, containing nearly half million network interactions. The file is provided as a Gzip file that we will download locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "f = urllib.urlretrieve (\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\", \"kddcup.data_10_percent.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an external table from the data file\n",
    "\n",
    "The downloaded file, \"kddcup.data_10_percent.gz\" will be located in the SparkProject directory.\n",
    "\n",
    "\n",
    "1. As we have done in the lab exercises, create a Hadoop directory \"/user/hive/warehouse/kddcup_10_percent\". This will the source directory for our external table.\n",
    "\n",
    "2. You now have to create an external table definition describing the data, as we did in the labs with \"CreatHousingExt.sql\". Be careful to use the correct data types. Use the file \"KDDCUP data schema.txt\" as a basis for your external table definition.\n",
    "\n",
    "You should do this task in the Hive editor by logging onto localhost:7180.\n",
    "\n",
    "3. Load the data into HDFS as we did in the labs (see the file \"LoadHousingData\" for the use of the -copyFromLocal command). \n",
    "\n",
    "Question 1 - Hadoop natively supports various compression algorithms. Does it natively support .gz files or do you have to unpack the file in order to load it into HDFS?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Hive to build your raw_data RDD\n",
    "\n",
    "Use the HiveContext sqlContext to get the data from your external table instead of loading it from the file.\n",
    "\n",
    "The format of the HiveContext is <rdd> = sqlContext.sql(\"<select statement>\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hive_raw_data  = sqlContext.sql(\"<your select statement>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How many records are in your RDD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hive_raw_data_count = <your code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_file = \"./kddcup.data_10_percent.gz\"\n",
    "raw_data = sc.textFile(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare your hive count to the record count from the data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data_count = <your code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting attack interactions using `subtract`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For illustrative purposes, imagine we already have our RDD with non attack (normal) interactions from some previous analysis.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normal_raw_data = raw_data.filter(lambda x: \"normal.\" in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "attack_raw_data = raw_data.subtract(normal_raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instead of using a python filter, extract the \"normal.\" interactions using hive\n",
    "\n",
    "Hint - use a \"where\" predicate in your select statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normal_raw_data_hive = sqlContext.sql(\"<your select statement>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain attack interactions by subtracting normal ones from the original unfiltered RDD as follows.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the attack interactions\n",
    "\n",
    "We can extract attack interactions by using the subtract method as above. Or - we can use a hive statement to extract the attack interactins directly. Again, use a \"where\" predicate in your select statement \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attack_raw_data_hive = slContext.sql(\"<your select statement>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some counts to check our results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "# count all\n",
    "t0 = time()\n",
    "raw_data_count = raw_data.count()\n",
    "tt = time() - t0\n",
    "print \"All count in {} secs\".format(round(tt,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count normal\n",
    "t0 = time()\n",
    "normal_raw_data_count = normal_raw_data.count()\n",
    "tt = time() - t0\n",
    "print \"Normal count in {} secs\".format(round(tt,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count attacks\n",
    "t0 = time()\n",
    "attack_raw_data_count = attack_raw_data.count()\n",
    "tt = time() - t0\n",
    "print \"Attack count in {} secs\".format(round(tt,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"There are {} normal interactions and {} attacks, \\\n",
    "from a total of {} interactions\".format(normal_raw_data_count,attack_raw_data_count,raw_data_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the results you have extracted using Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count all\n",
    "t0 = time()\n",
    "hive_raw_data_count = <...your code...>\n",
    "tt = time() - t0\n",
    "print \"All hive count in {} secs\".format(round(tt,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count normal\n",
    "t0 = time()\n",
    "normal_raw_data_count_hive = <...your code...>\n",
    "tt = time() - t0\n",
    "print \"Normal hive count in {} secs\".format(round(tt,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count attacks\n",
    "t0 = time()\n",
    "attack_raw_data_count_hive = <...your code...>\n",
    "tt = time() - t0\n",
    "print \"Attack count in {} secs\".format(round(tt,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"For our Hive data - there are {} normal interactions and {} attacks, \\\n",
    "from a total of {} interactions\".format(normal_raw_data_count_hive,attack_raw_data_count_hive,hive_raw_data_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have two RDDs, one with normal interactions and another one with attacks.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Protocol and service combinations using `cartesian`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the Cartesian product between two RDDs by using the `cartesian` transformation. It returns all possible pairs of elements between two RDDs. In our case we will use it to generate all the possible combinations between service and protocol in our network interactions.  \n",
    "\n",
    "First of all we need to isolate each collection of values in two separate RDDs. For that we will use `distinct` on the CSV-parsed dataset. From the [dataset description](http://kdd.ics.uci.edu/databases/kddcup99/kddcup.names) we know that protocol is the second column and service is the third (tag is the last one and not the first as appears in the page).   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So first, let's get the protocols.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csv_data = raw_data.map(lambda x: x.split(\",\"))\n",
    "protocols = csv_data.map(lambda x: x[1]).distinct()\n",
    "protocols.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do the same for services.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "services = csv_data.map(lambda x: x[2]).distinct()\n",
    "services.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A longer list in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do the cartesian product.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "product = protocols.cartesian(services).collect()\n",
    "print \"There are {} combinations of protocol X service\".format(len(product))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, for such small RDDs doesn't really make sense to use Spark cartesian product. We could have perfectly collected the values after using `distinct` and do the cartesian product locally. Moreover, `distinct` and `cartesian` are expensive operations so they must be used with care when the operating datasets are large. \n",
    "\n",
    "# Hive can simplify this process\n",
    "A cartesian product can be generated from one Hive statement and the cartesian RDD can be created in one statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "You simply select the distinct values of protocols and services using a nested queries and join the two results without a selection predicate.\n",
    "\n",
    "An example\n",
    "\n",
    "select a.firstname, b.lastname\n",
    "from\n",
    "/* first nested query with the table alias a */\n",
    "(select distinct customer_fname as firstname from customers) a JOIN\n",
    "/* second nested query with the table alias b */\n",
    "(select distinct customer_lname as lastname from customers) b\n",
    "\n",
    "Notice there is no join predicate specified by a \"where...\" clause. This is what triggers the cartesian join.\n",
    "\n",
    "Rewite the above cells to generate a full combination of protocols and services from the Hive table you have created.\n",
    "\n",
    "Notice in the cell below how to generate a multiline SQL statement and pass that statement as a variable. We usually do this for readability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cartesianSQL = (\"select ...<your code>... \"\n",
    "                \"...<your code>... \"\n",
    "                \"...<your code>...\")\n",
    "cartesian_join = sqlContext.sql(cartesianSQL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Display your results. They should be the same as the product calculation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Using Hive, there are {} combinations of protocol X service\".format(...<your code>...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
