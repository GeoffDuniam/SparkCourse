{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data aggregations on RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Introduction to Spark with Python, by Jose A. Dianes](https://github.com/jadianes/spark-py-notebooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can aggregate RDD data in Spark by using three different actions: `reduce`, `fold`, and `aggregate`. The last one is the more general one and someway includes the first two.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data and creating the RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did in our first notebook, we will use the reduced dataset (10 percent) provided for the [KDD Cup 1999](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html), containing nearly half million nework interactions. The file is provided as a Gzip file that we will download locally.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "f = urllib.urlretrieve (\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\", \"kddcup.data_10_percent.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_file = \"./kddcup.data_10_percent.gz\"\n",
    "raw_data = sc.textFile(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting interaction duration by tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both `fold` and `reduce` take a function as an argument that is applied to two elements of the RDD. The `fold` action differs from `reduce` in that it gets and additional initial *zero value* to be used for the initial call. This value should be the identity element for the function provided.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, imagine we want to know the total duration of our interactions for normal and attack interactions. We can use `reduce` as follows.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# parse data\n",
    "csv_data = raw_data.map(lambda x: x.split(\",\"))\n",
    "\n",
    "# separate into different RDDs\n",
    "normal_csv_data = csv_data.filter(lambda x: x[41]==\"normal.\")\n",
    "attack_csv_data = csv_data.filter(lambda x: x[41]!=\"normal.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function that we pass to `reduce` gets and returns elements of the same type of the RDD. If we want to sum durations we need to extract that element into a new RDD.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normal_duration_data = normal_csv_data.map(lambda x: int(x[0]))\n",
    "attack_duration_data = attack_csv_data.map(lambda x: int(x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can reduce these new RDDs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_normal_duration = normal_duration_data.reduce(lambda x, y: x + y)\n",
    "total_attack_duration = attack_duration_data.reduce(lambda x, y: x + y)\n",
    "\n",
    "print \"Total duration for 'normal' interactions is {}\".\\\n",
    "    format(total_normal_duration)\n",
    "print \"Total duration for 'attack' interactions is {}\".\\\n",
    "    format(total_attack_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can go further and use counts to calculate duration means.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normal_count = normal_duration_data.count()\n",
    "attack_count = attack_duration_data.count()\n",
    "\n",
    "print \"Mean duration for 'normal' interactions is {}\".\\\n",
    "    format(round(total_normal_duration/float(normal_count),3))\n",
    "print \"Mean duration for 'attack' interactions is {}\".\\\n",
    "    format(round(total_attack_duration/float(attack_count),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a first (and too simplistic) approach to identify attack interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A better way, using `aggregate`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `aggregate` action frees us from the constraint of having the return be the same type as the RDD we are working on. Like with `fold`, we supply an initial zero value of the type we want to return. Then we provide two functions. The first one is used to combine the elements from our RDD with the accumulator. The second function is needed to merge two accumulators. Let's see it in action calculating the mean we did before.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normal_sum_count = normal_duration_data.aggregate(\n",
    "    (0,0), # the initial value\n",
    "    (lambda acc, value: (acc[0] + value, acc[1] + 1)), # combine value with acc\n",
    "    (lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])) # combine accumulators\n",
    ")\n",
    "\n",
    "print \"Mean duration for 'normal' interactions is {}\".\\\n",
    "    format(round(normal_sum_count[0]/float(normal_sum_count[1]),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous aggregation, the accumulator first element keeps the total sum, while the second element keeps the count. Combining an accumulator with an RDD element consists in summing up the value and incrementing the count. Combining two accumulators requires just a pairwise sum.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same with attack type interactions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "attack_sum_count = attack_duration_data.aggregate(\n",
    "    (0,0), # the initial value\n",
    "    (lambda acc, value: (acc[0] + value, acc[1] + 1)), # combine value with acc\n",
    "    (lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])) # combine accumulators\n",
    ")\n",
    "\n",
    "print \"Mean duration for 'attack' interactions is {}\".\\\n",
    "    format(round(attack_sum_count[0]/float(attack_sum_count[1]),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, we're going to calculate the same results using Hive\n",
    "\n",
    "Inbuilt function in Hive will make this task very easy. Here is the Apache language reference for Hive.\n",
    "https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF\n",
    "\n",
    "If we examine the built in aggregate functions, we can see functions for sums, averages, percentiles and counts. So, for notmal and attach interactions we can calculate the total duration, number of interactions and mean duration of each interaction in one statement.\n",
    "\n",
    "If we use a group statement, we will collect the statistics we need for both normal and attack interactions in one operation.\n",
    "\n",
    "The Hive QL pseudocode you will need is\n",
    "\n",
    "select <interaction type>,count(<duration>), average(<duration>) and sum(<duration>)\n",
    "from <your external table you created in the previous exervise>\n",
    "group by <interaction_type>.\n",
    "\n",
    "Remember - ^^^ that is pseudocode! you will have to change it to work against the table you created from the previous notebook exercise. You will also have to check and confirm the proper names of the in built aggregate functions you will need to use.\n",
    "\n",
    "*HINT* You can always use the Hive uditor to test your SQL statement to make sure it's giving you the results you want\n",
    "\n",
    "This will result in an RDD that looks like this -\n",
    "\n",
    "[Row(interaction_type=u'normal.', mySum=47402177, mycount=7610, myavg=6228.93258869908),\n",
    " Row(interaction_type=u'attack.', mySum=51508051, mycount=8275, myavg=6224.537885196374)]\n",
    " \n",
    "what you have to do now is separate the normal and attack interactions, and extract the individual values.\n",
    "\n",
    "so, you'd create two RDDs from your data, one for normal, and one for attack.\n",
    "\n",
    "Example, for normal, use a filter on the first value of the RDD (index 0) which is the interaction type, ie.\n",
    "\n",
    "normalData=data.filter(data[0] == 'normal.')\n",
    "\n",
    "This will give you the aggregated data for noral interactions, ie.\n",
    "\n",
    "[Row(interaction_type=u'normal.', mySum=47402177, mycount=7610, myavg=6228.93258869908)]\n",
    "\n",
    "Now you extract the values from the \"normal\" RDD into variables.\n",
    "\n",
    "Extracting a specific column from a RDDm you use the \"select\" function, i.e.\n",
    "\n",
    "normalCount = normalData.select(\"mycount\").take(1)\n",
    "\n",
    "Notice that the RDD has the column definitions and we are using the \"mycount\" column to get the count of normal interactions. Note that we also need to use the \"take\" function to get the actual value, and not just the definition.\n",
    "\n",
    "We now have a list key-value pair that looks like this.\n",
    "\n",
    "[Row(mycount=7610)]\n",
    "\n",
    "what we want to do is extract the value so we can use it in a variable, so we get the value with the \__getitem__ method, i.e.\n",
    "\n",
    "normalCount = normalCount[0].__getitem__(\"mycount\")\n",
    "                                            ^^^      take note, we are using the column name\n",
    "\n",
    "NormalCount now is a variable with the value 7610.\n",
    "\n",
    "We can combine these operations into one, but it's a little harder to read. i.e.\n",
    "\n",
    "normalCount = normalData.select(\"mycount\").take(1)[0].__getitem__(\"mycount\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Create the HiveQL RDD containing your results\n",
    "\n",
    "You want an RDD with two lines, with count, average and sum data for the duration for normal and attack traffic; you'll need to use a GROUP BY statement.\n",
    "\n",
    "*HINT* Use a multiline SQL statement for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlStmt = (\"select ...<your code>... \"\n",
    "                \"...<your code>... \"\n",
    "                \"...<your code>...\"\n",
    "                \" from .....\"\n",
    "                \"group by .....\"\n",
    "          )\n",
    "\n",
    "data = sqlContext.sql(sqlStmt)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Extract normal and attack data into separate RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normalData=data.filter(data[0] == '<...normal tag from file...>') # <== you will need to identify this from the file\n",
    "attackData=data.filter(<...your code here...>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. Extract the individual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normal values\n",
    "normalSum = normalData.select(...<your code>...).<your code>\n",
    "normalCount= normalData.select(...<your code>...).<your code>\n",
    "normalAverage = ...<your code>...\n",
    "\n",
    "# Attack values\n",
    "attackSum = ...<your code>...\n",
    "attackCount = ...<your code>...\n",
    "attackAverage = ...<your code>..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display the results and compare "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"Total duration for 'normal' interactions is {}\".\\\n",
    "    format(...<your code>...)\n",
    "    \n",
    "print \"Total duration for 'attack' interactions is {}\".\\\n",
    "    format(...<your code>...)\n",
    "\n",
    "print \"Mean duration for 'normal' interactions is {}\".\\\n",
    "    format(...<your code>...)\n",
    "\n",
    "print \"Mean duration for 'attack' interactions is {}\".\\\n",
    "    format(...<your code>...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
